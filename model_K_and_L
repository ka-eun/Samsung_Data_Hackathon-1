from preprocessing import category_preprocessing, shuffleList, deleteColumn
from keras.layers.core import Dense, Activation, Dropout
from keras.callbacks import EarlyStopping
from keras.layers import concatenate, Input, BatchNormalization, PReLU
from keras.models import Model, Sequential
from keras.utils.vis_utils import plot_model
import numpy as np
import tensorflow as tf
import matplotlib.pyplot as plt
from collections import Counter
import operator
import random
from math import exp
from pprint import pprint


# SEED = 448
# np.random.seed(SEED)
# tf.set_random_seed(SEED)
# random.seed(SEED)


def plot_hist(hist):
    fig, loss_ax = plt.subplots()

    acc_ax = loss_ax.twinx()

    loss_ax.plot(hist.history['loss'], 'y', label='train loss')
    loss_ax.plot(hist.history['val_loss'], 'r', label='val loss')

    acc_ax.plot(hist.history['acc'], 'b', label='train acc')
    acc_ax.plot(hist.history['val_acc'], 'g', label='val acc')

    loss_ax.set_xlabel('epoch')
    loss_ax.set_ylabel('loss')
    acc_ax.set_ylabel('accuray')

    loss_ax.legend(loc='upper left')
    acc_ax.legend(loc='lower left')

    plt.show()


def norm_list(input):
    return [1 for _ in range(len(input))] if min(input) == max(input) else [(i - min(input)) / (max(input) - min(input)) for i in input]


#리스트 inputs을 test,train,validation 세 리스트로 분화하는 함수
def separateSet(_inputs, _outputs):
    # inputs: attribute에 따라 생성한 list
    inputs = []
    for _ in range(len(_inputs[0])):
        #inputs의 원소로 attr의 개수만큼의 리스트를 만듬
        inputs.append([])

    for _input in _inputs:
        for i, elem in enumerate(_input):
            #_inputs 속성별로 재분류해서 inputs에
            inputs[i].append(elem)

    # train, test, validation
    inputs_by_attr = []

    for _ in range(3):
        inputs_by_attr.append([])

    for i, col in enumerate(inputs):
        n = len(col)
        inputs_by_attr[0].append(col[:int(n / 10)])  # test
        inputs_by_attr[1].append(col[int(n / 10):n - int(n / 10)])  # train
        inputs_by_attr[2].append(col[n - int(n / 10):])  # validation

    """
    """
    outputs = []
    for _ in range(len(_outputs[0])):
        outputs.append([])

    for _output in _outputs:
        for i, elem in enumerate(_output):
            outputs[i].append(elem)

    # train, test, validation
    outputs_by_attr = []

    for _ in range(3):
        outputs_by_attr.append([])

    for i, col in enumerate(outputs):
        n = len(col)
        outputs_by_attr[0].append(col[:int(n / 10)])  # test
        outputs_by_attr[1].append(col[int(n / 10):n - int(n / 10)])  # train
        outputs_by_attr[2].append(col[n - int(n / 10):])  # validation

    """
    """
    return inputs, outputs, \
           inputs_by_attr[0], inputs_by_attr[1], inputs_by_attr[2], \
           outputs_by_attr[0], outputs_by_attr[1], outputs_by_attr[2]


def majority(votes):
    #만들어진 모델들에 대해서 다수결 투표하는 함수
    # 투표 개수를 세어서 표결수가 큰 순서대로 정렬한 후 가장 많은 표를 받은 키값
    # 투표 개수를 세어서 표결수가 큰 순서대로 정렬한 후 가장 많은 표를 받은 키값의 표결수를 투표자 수로 나눔. 즉 비율
    return sorted(Counter(votes).items(), key=operator.itemgetter(1), reverse=True)[0][0], \
           (sorted(Counter(votes).items(), key=operator.itemgetter(1), reverse=True)[0][1] / len(votes))


def createModel(inputs, outputs):
    # inputs
    models = []

    # inputs의 열 별로
    for input in inputs:
        # 행은 각 속성의 분류 개수(예:주간,아간은 2)이고 열은 개수에 맞춰서 설정된 행렬
        model = Input(shape=(len(input[0]),))
        models.append(model)

    # more layers for each one-hot encoding vector
    _models = []
    for i, model in enumerate(models):
        """
        model = Dense(len(inputs[i][0]), kernel_initializer='he_normal')(model)
        model = BatchNormalization()(model)
        model = Activation('elu')(model)
        """

        # collect refined model
        _models.append(model)

    # merge
    x = concatenate(_models)

    x = Dense(400, kernel_initializer='glorot_normal')(x)
    x = BatchNormalization()(x)
    x = Activation('tanh')(x)
    x = Dropout(0.5)(x)

    x = Dense(400, kernel_initializer='glorot_normal')(x)
    x = BatchNormalization()(x)
    x = Activation('tanh')(x)
    x = Dropout(0.5)(x)

    # output 1
    o1 = Dense(400, kernel_initializer='glorot_normal')(x)
    o1 = BatchNormalization()(o1)
    o1 = Activation('tanh')(o1)
    o1 = Dropout(0.5)(o1)

    o1 = Dense(len(outputs[0][0]))(o1)
    o1 = Activation('softmax', name='yu-jeong')(o1)

    # output 2
    o2 = Dense(400, kernel_initializer='glorot_normal')(x)
    o2 = BatchNormalization()(o2)
    o2 = Activation('tanh')(o2)
    o2 = Dropout(0.5)(o2)

    o2 = Dense(len(outputs[1][0]))(o2)
    o2 = Activation('softmax', name='ka-eun')(o2)

    return Model(inputs=models, outputs=[o1, o2])


if __name__ == "__main__":
    _inputs, _outputs, _ = category_preprocessing()  # 범주형 데이터 리스트, 사람 수 데이터 리스트, 벡터화 dictionary

    # delete columns
    _inputs = deleteColumn(_inputs, [1, 3, 8, 10])  # 1, 3, 8, 10

    # make list
    tmp = []
    for _input in _inputs:
        ttmp = []
        nums = []
        for i, elem in enumerate(_input):
            if i < 1:
                ttmp.append(elem)
            elif i < 5:
                nums.append(elem)
            elif i == 5:
                ttmp.append(norm_list(nums))
                ttmp.append(elem)
            else:
                ttmp.append(elem)

        tmp.append(ttmp)

    _inputs, _outputs = shuffleList(tmp, _outputs)  # 데이터 리스트가 고루 섞이도록 _inputs와 _outputs를 함께 섞음
    inputs, outputs, input_test, input_train, input_val, output_test, output_train, output_val \
        = separateSet(_inputs, _outputs)  # 범주형 데이터와 사람 수 데이터를 각각 test, train, validate를 위해 분류

    # for ensemble model
    num_models = 10
    models = []

    # model의 개수만큼 model 생성
    for i in range(num_models):
        """
        create model
        """
        model = createModel(inputs, outputs)
        models.append(model)

    scores = []
    predicts = []

    # 각 model에 대한 training
    for j, model in enumerate(models):
        """
        training
        """
        model.compile(loss="categorical_crossentropy", optimizer="adam", metrics=['accuracy'])
        model.summary()

        # early stopping
        # val_acc값이 5번 동안 향상되지 않으면 해당 model의 학습을 중단
        early_stopping = EarlyStopping(monitor='val_loss', patience=10, verbose=2)

        # train
        # model의 학습 이력 정보로 train의 loss와 accuracy, val의 loss와 accuracy 값을 받음
        hist = model.fit([np.array(i) for i in input_train], [np.array(i) for i in output_train],
                         epochs=1000, batch_size=pow(2, 13),
                         validation_data=([np.array(i) for i in input_val], [np.array(i) for i in output_val]),
                         callbacks=[early_stopping],
                         verbose=2)

        # plot_hist(hist)

        """
        test
        """
        # model의 성능 평가
        score = model.evaluate([np.array(i) for i in input_test], [np.array(i) for i in output_test], verbose=0)
        print('complete: %s = %.2f%%' % (model.metrics_names[3], score[3] * 100))
        print('complete: %s = %.2f%%' % (model.metrics_names[4], score[4] * 100))

        """
        predict
        """
        # _preds = model.predict([np.array(i) for i in input_test])

        """
        for i, row in enumerate(_preds):
            row = list(row)
            pred = row.index(max(row))
            real = output_test[i].index(max(output_test[i]))

            print("pred:", pred, '|', "real:", real)
        """

        break
